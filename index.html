<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Harry Freeman</title>
  
  <meta name="author" content="Harry Freeman">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Harry Freeman</name>
              </p>
              <p>
                I am a first year PhD student in the <a href="http://www.ri.cmu.edu" target="_blank">Robotics Institute</a> at Carnegie Mellon University 
                supervised by Professor George Kantor. 
                My research interests are in robot learning, semantic scene understanding, and data-efficiency. I seek to explore generalizable
                and adaptable methods that allow robots to reason about objects in their surrounding in order to perform complex tasks in diverse 
                and unstructured environments.
              </p>
              <p>
                I completed my Master of Science in Robotics at CMU working with 
                Professor <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>. My research focused on computer vision,
                3D reconstruction, and next-best-view planning for phenotyping small crops in agriculture.
              </p>
              <p>
                Prior to CMU, I was a senior embedded software engineer for Amazon Web Services in their AI Devices division.
                I worked on the <a href="https://aws.amazon.com/panorama/" target="_blank">AWS Panorama</a> appliance that brings computer vision and machine learning capabilities to IP cameras.
                I completed my undergraduate degree at Cornell University where I studied Electrical and Computer Engineering.
              </p>
              <p style="text-align:center">
                <a href="mailto:hfreeman@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/hfreeman_cv.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2HY8KIAAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/harrynvfreeman/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/hfreecmu" target="_blank">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hfreeman_cmu.jpeg" target="_blank"><img style="width:80%;max-width:80%" alt="profile photo" src="images/hfreeman_cmu.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-left:20px;padding-right:20px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading>Research Publications</heading>
              <p style="padding:0px;margin:0px;">(representative papers are <span class="highlight">highlighted</span>)</p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px">
              <h1><strong>Journals</strong></h1>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/fruitlet_size_resize.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.01506" target="_blank">
                <papertitle>Autonomous Apple Fruitlet Sizing and Growth Rate Tracking using Computer Vision</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://scholar.google.com/citations?user=GcwJYdEAAAAJ&hl=en" target="_blank">Mohamad Qadri</a>, 
              <a href="https://scholar.google.com/citations?user=4ftPKy0AAAAJ&hl=en" target="_blank">Abhisesh Silwal</a>,
              Paul O'Connor,
              <a href="https://www.cs.cmu.edu/~zbr/" target="_blank">Zachary Rubinstein</a>,
              <a href="https://stockbridge.cns.umass.edu/daniel-cooley?_gl=1*gr02k3*_ga*MzE3NTg5NzczLjE2Njk2NjA0Mjk.*_ga_21RLS0L7EB*MTY3MDA5MDA2My4yLjEuMTY3MDA5MDIzOC4wLjAuMA..&_ga=2.40665762.1082394084.1670090063-317589773.1669660429" target="_blank">Daniel Cooley</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>In Submission at IEEE Transactions on Robotics (T-RO)</em>
              <br>
              <a href="https://arxiv.org/abs/2212.01506" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/pdf/2212.01506.pdf" target="_blank">[PDF]</a>
              <a href="https://www.youtube.com/watch?v=eoPMhOJpM0U" target="_blank">[Video]</a>
              <p></p>
              <p>
                We develop a computer vision-based method to size and track the growth rates of apple fruitlets. Fruitlets are sized and temporally associated
                using a combination of deep learning-based and classical methods. 
              </p>
            </td>
          </tr>	
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px">
              <h1><strong>Peer-reviewed Conferences</strong></h1>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/nbv_comp.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2309.13669" target="_blank">
                <papertitle>Autonomous Apple Fruitlet Sizing with Next Best View Planning</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>Accepted to International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.13669" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/pdf/2309.13669.pdf" target="_blank">[PDF]</a>
              <a href="https://www.youtube.com/watch?v=Qjx6-qYbwrs" target="_blank">[Video]</a>
              <p></p>
              <p>
                Developed a novel next-best-view planning approach to enable a 7 DoF robotic arm to autonomously capture images of apple fruitlets. 
                Utilized a coarse and fine dual-map representation along with an attention-guided information gain formulation to determine the next best camera pose. 
                Presented a robust estimation and graph clustering approach to associate fruit detections across images in the presence of wind and sensor error. 
              </p>
            </td>
          </tr>	

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/sorghum_model_crop.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.07748" target="_blank">
                <papertitle>3D Reconstruction-Based Seed Counting of Sorghum Panicles for Agricultural Inspection</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://franzericschneider.github.io/" target="_blank">Eric Schneider</a>, 
              <a href="https://scholar.google.com/citations?hl=en&user=18tZI8gAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Chung Hee Kim</a>,
              <a href="http://www.moonrobotics.org/" target="_blank">Moonyoung Lee</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2211.07748" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/pdf/2211.07748.pdf" target="_blank">[PDF]</a>
              <a href="https://drive.google.com/drive/u/0/folders/1MXGWW-NE_rNC0Taw4pzL_QfbfEsyP-wG" target="_blank">[Dataset]</a>
              <a href="https://www.youtube.com/watch?v=WRiH6tLeXcg&feature=youtu.be" target="_blank">[Video]</a>
              <p></p>
              <p>
                We develop a method for creating high-quality 3D models of sorghum panicles to non-destructively estimate seed counts.
                This is acheived using seeds as semantic 3D landmarks for global registration and a novel density-based clustering approach.
                Additionally, we present an unsupervised metric to assess point cloud
                reconstruction quality in the absence of ground truth.
              </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/cinematography_crop_resize.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2108.03936" target="_blank">
                <papertitle>3D Human Reconstruction in the Wild with Collaborative Aerial Cameras</papertitle>
              </a>
              <br>
              <a href="https://cherieho.com/" target="_blank">Cherie Ho</a>, 
              <a href="https://andrewjong.github.io/" target="_blank">Andrew Jong</a>, 
              <strong>Harry Freeman</strong>, 
              <a href="https://scholar.google.com/citations?hl=en&user=8Yd0UkIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Rohan Rao</a>,
              <a href="http://rogeriobonatti.com/" target="_blank">Rogerio Bonatti</a>,
              <a href="https://theairlab.org/team/sebastian/" target="_blank">Sebastian Scherer</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.03936" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/abs/2108.03936.pdf" target="_blank">[PDF]</a>
              <a href="https://youtu.be/jxt91vx0cns" target="_blank">[Video]</a>
              <p></p>
              <p>
                We build a real-time aerial system for multi-camera control 
                that can reconstruct human motions in natural environments without the use of special-purpose markers. 
                This is acheived with a multi-robot coordination scheme that maintains the optimal flight formation for target reconstruction quality amongst obstacles.
              </p>
            </td>
          </tr>	
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px">
              <h1><strong>Workshops</strong></h1>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/sim_fruitlet_size.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=DcHdI9-gK3" target="_blank">
                <papertitle>Towards Autonomous Apple Fruitlet Sizing with Next Best View Planning</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>AI for Agriculture and Food Systems (AIAFS)</em>, 2023
              <br>
              <a href="https://openreview.net/forum?id=DcHdI9-gK3" target="_blank">[PDF]</a>
              <p></p>
              <p>
                We develop a next-best-view planning approach to capture images of and size apple fruitlets. Our planner utilizes
                both coarse and fine octrees to map the environment and to calculate the information gain of sampled viewpoints. 
                Fruitlet sizing is performed by reprojecting extracted fruitlet surfaces onto 2D images and fitting ellipses.
              </p>
            </td>
          </tr>	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/semantic_sorghum.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=_1K0EZJQCy" target="_blank">
                <papertitle>Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=GcwJYdEAAAAJ&hl=en" target="_blank">Mohamad Qadri</a>,
              <strong>Harry Freeman</strong>, 
              <a href="https://franzericschneider.github.io/" target="_blank">Eric Schneider</a>, 
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>AI for Agriculture and Food Systems (AIAFS)</em>, 2022
              <br>
              <a href="https://openreview.net/forum?id=_1K0EZJQCy" target="_blank">[PDF]</a>
              <a href="https://www.youtube.com/watch?v=uCM7N2tOVVs&feature=youtu.be" target="_blank">[Video]</a>
              <p></p>
              <p>
                We demonstrate how the use of semantics and environmental priors can help in constructing accurate 3D maps for downstream agricultural tasks
                with the target application of phenotyping Sorghum.
              </p>
            </td>
          </tr>	
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
          <tr>
            <td style="padding-left:20px;padding-right:20px">
              <h1><strong>Thesis</strong></h1>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/nbv_bumblebee.jpg' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ri.cmu.edu/publications/computer-vision-based-phenotyping-in-agriculture-leveraging-semantic-information-for-non-destructive-small-crop-analysis/" target="_blank">
                <papertitle>Computer Vision-Based Phenotyping in Agriculture: Leveraging Semantic Information for Non-Destructive Small Crop Analysis</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>              <br>
              <em>Master's Thesis</em>, 2023
              <br>
              <a href="https://www.ri.cmu.edu/publications/computer-vision-based-phenotyping-in-agriculture-leveraging-semantic-information-for-non-destructive-small-crop-analysis/" target="_blank">[PDF]</a>
              <p></p>
              <p>
                We present computer vision-based methods to non-destructively measure phenotypes 
                of smaller grains and fruit, specifically sorghum seed counts and apple fruitlet sizes. We do this by leveraging semantic 
                information to improve tasks such as localization, association, and viewpoint planning. 
              </p>
            </td>
          </tr>	
        </tbody></table>
        
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
              <p>
                A few selected projects from a mix of academic and personal.
               </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/4_with_obstacle_mobile_arm_moving_ball.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/16_884_Project_Final_Report.pdf" target="_blank">
                <papertitle>Learning a Unified Policy for Locomotion with Eye-in-Hand Perception</papertitle>
              </a>
              <br>
              Deep Learning for Robotics Final Project (December 2022)
              <br>
              <a href="data/16_884_Project_Final_Report.pdf" target="_blank">[PDF]</a>
              <a href="https://github.com/hfreecmu/IsaacGymEnvs/tree/pentapede" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Applied deep reinforcement learning to learn a unified policy that controls a quadruped mounted with a camera 
                on a mobile arm with the task of tracking a moving target.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/PPO_3_agent.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Robostats_course_project_final_report.pdf" target="_blank">
                <papertitle>Online Informative Path Planning for Drone Mapping with Reinforcement Learning</papertitle>
              </a>
              <br>
              Introduction to Robot Learning (Robostats) Final Project (December 2022)
              <br>
              <a href="data/Robostats_course_project_final_report.pdf" target="_blank">[PDF]</a>
              <a href="https://github.com/russelldj/informative-path-planning-toolkit/tree/robostats_final_project" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Applied deep reinforcement learning techniques towards informative path planning for drone mapping.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/sparse_recon_crop.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Learning_3D_vision_Final_Report.pdf" target="_blank">
                <papertitle>Sparse View Mesh Reconstruction of Plants</papertitle>
              </a>
              <br>
              Learning for 3D Vision Final Project (May 2022)
              <br>
              <a href="data/Learning_3D_vision_Final_Report.pdf" target="_blank">[PDF]</a>
              <a href="https://github.com/russelldj/leaf_reconstruction" target="_blank">[Data Repo]</a>
              <a href="https://github.com/hfreecmu/NeuS" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Applied NeuS and metalearning towards watertight mesh reconstruction of plants from sparse viewpoints. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/gan_augment.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.andrew.cmu.edu/course/16-726-sp22/projects/hfreeman/project/" target="_blank">
                <papertitle>GANs for Coarse Style and Scene Data Augmentation</papertitle>
              </a>
              <br>
              Learning-Based Image Synthesis Final Project (May 2022)
              <br>
              <a href="https://www.andrew.cmu.edu/course/16-726-sp22/projects/hfreeman/project/" target="_blank">[Project Webpage]</a>
              <a href="https://github.com/hfreecmu/swapping-autoencoder-pytorch" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Utilized Generative-Adversarial Networks and Swapping AutoEncoders for coarse style and scene data augmentation.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/hog.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="face_detection.html" target="_blank">
                <papertitle>Real Time Face Detection</papertitle>
              </a>
              <br>
               Personal Project (2019) 
              <br>
              <a href="face_detection.html" target="_blank">[Project Summary]</a>
              <p></p>
              <p>
                Built a real-time face detection system in Cython and C using custom implementation of Histogram of Oriented Gradients.
              </p>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                <br>
              </p>
              <td align=right>
                Template from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>. Last updated: August 20th 2023.</td>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
