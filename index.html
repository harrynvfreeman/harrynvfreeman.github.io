<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Harry Freeman</title>
  
  <meta name="author" content="Harry Freeman">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Harry Freeman</name>
              </p>
              <p>
                I am a PhD student in the <a href="http://www.ri.cmu.edu" target="_blank">Robotics Institute</a> at
                <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a>
                advised by Professor George Kantor <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">Carnegie Mellon University</a>
                in the <a href="https://labs.ri.cmu.edu/kantorlab/" target="_blank">Kantor Lab</a>.
              </p>
              <p>
                I received my B.S. in ECE from Cornell University and my Masters in Robotics at CMU, where I worked on computer vision,
                3D reconstruction, and next-best-view planning to phenotype small crops in agriculture. Prior to CMU, I was a senior embedded software engineer for Amazon Web Services in their 
                AI Devices division working on <a href="https://aws.amazon.com/panorama/" target="_blank">AWS Panorama</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:hfreeman@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/hfreeman_cv.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2HY8KIAAAAAJ&hl=en&authuser=1" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/harrynvfreeman/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/hfreecmu" target="_blank">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/hfreeman_cmu.jpeg" target="_blank"><img style="width:80%;max-width:80%" alt="profile photo" src="images/hfreeman_cmu.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-left:20px;padding-right:20px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>My research interests lie at the intersection of 3D reconstruction, robotic manipulation, and learning from human demonstration. I wish to 
                enable robots to perform complex tasks in diverse and unstructured environments such as agriculture. I am currently working on 
                cross-embodiment learning from human demonstration utilizing gaussian splatting.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding-top:20px;padding-left:20px;padding-right:20px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/spatio_temporal_pipeline.jpg' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.03200" target="_blank">
                <papertitle>Transformer-Based Spatio-Temporal Association of Apple Fruitlets</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>In Submission at IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2503.03200" target="_blank">[arXiv]</a>
              <a href="http://arxiv.org/pdf/2503.03200" target="_blank">[PDF]</a>
              <a href="https://www.youtube.com/watch?v=_XxsPLjcGnw" target="_blank">[Video]</a>
              <p></p>
              <p>
                Created a method for temporal apple fruitlet association utilizing stereo images and transformers. 
                Able to achieve F1 matching accuracy of 92.4% on new dataset collected over 3 years of 3 different varietals. 
                We demonstrate that our transformer architecture is generalizable to other datasets and modalities.
              </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/nbv_comp.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2309.13669" target="_blank">
                <papertitle>Autonomous Apple Fruitlet Sizing with Next Best View Planning</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>Accepted to International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.13669" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/pdf/2309.13669.pdf" target="_blank">[PDF]</a>
              <a href="https://www.youtube.com/watch?v=Qjx6-qYbwrs" target="_blank">[Video]</a>
              <p></p>
              <p>
                Developed a novel next-best-view planning approach to enable a 7 DoF robotic arm to autonomously capture images of apple fruitlets. 
                Utilized a coarse and fine dual-map representation along with an attention-guided information gain formulation to determine the next best camera pose. 
                Presented a robust estimation and graph clustering approach to associate fruit detections across images in the presence of wind and sensor error. 
              </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/sorghum_model_crop.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.07748" target="_blank">
                <papertitle>3D Reconstruction-Based Seed Counting of Sorghum Panicles for Agricultural Inspection</papertitle>
              </a>
              <br>
              <strong>Harry Freeman</strong>, 
              <a href="https://franzericschneider.github.io/" target="_blank">Eric Schneider</a>, 
              <a href="https://scholar.google.com/citations?hl=en&user=18tZI8gAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Chung Hee Kim</a>,
              <a href="http://www.moonrobotics.org/" target="_blank">Moonyoung Lee</a>,
              <a href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/" target="_blank">George Kantor</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2211.07748" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/pdf/2211.07748.pdf" target="_blank">[PDF]</a>
              <a href="https://drive.google.com/drive/u/0/folders/1MXGWW-NE_rNC0Taw4pzL_QfbfEsyP-wG" target="_blank">[Dataset]</a>
              <a href="https://www.youtube.com/watch?v=WRiH6tLeXcg&feature=youtu.be" target="_blank">[Video]</a>
              <p></p>
              <p>
                We develop a method for creating high-quality 3D models of sorghum panicles to non-destructively estimate seed counts.
                This is acheived using seeds as semantic 3D landmarks for global registration and a novel density-based clustering approach.
                Additionally, we present an unsupervised metric to assess point cloud
                reconstruction quality in the absence of ground truth.
              </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/cinematography_crop_resize.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2108.03936" target="_blank">
                <papertitle>3D Human Reconstruction in the Wild with Collaborative Aerial Cameras</papertitle>
              </a>
              <br>
              <a href="https://cherieho.com/" target="_blank">Cherie Ho</a>, 
              <a href="https://andrewjong.github.io/" target="_blank">Andrew Jong</a>, 
              <strong>Harry Freeman</strong>, 
              <a href="https://scholar.google.com/citations?hl=en&user=8Yd0UkIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Rohan Rao</a>,
              <a href="http://rogeriobonatti.com/" target="_blank">Rogerio Bonatti</a>,
              <a href="https://theairlab.org/team/sebastian/" target="_blank">Sebastian Scherer</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.03936" target="_blank">[arXiv]</a>
              <a href="https://arxiv.org/abs/2108.03936.pdf" target="_blank">[PDF]</a>
              <a href="https://youtu.be/jxt91vx0cns" target="_blank">[Video]</a>
              <p></p>
              <p>
                We build a real-time aerial system for multi-camera control 
                that can reconstruct human motions in natural environments without the use of special-purpose markers. 
                This is acheived with a multi-robot coordination scheme that maintains the optimal flight formation for target reconstruction quality amongst obstacles.
              </p>
            </td>
          </tr>	
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Other Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/4_with_obstacle_mobile_arm_moving_ball.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/16_884_Project_Final_Report.pdf" target="_blank">
                <papertitle>Learning a Unified Policy for Locomotion with Eye-in-Hand Perception</papertitle>
              </a>
              <br>
              Deep Learning for Robotics Final Project (December 2022)
              <br>
              <a href="data/16_884_Project_Final_Report.pdf" target="_blank">[PDF]</a>
              <a href="https://github.com/hfreecmu/IsaacGymEnvs/tree/pentapede" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Applied deep reinforcement learning to learn a unified policy that controls a quadruped mounted with a camera 
                on a mobile arm with the task of tracking a moving target.
              </p>
            </td>
          </tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video  width=100% muted autoplay loop>
                <source src="images/sparse_recon_crop.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Learning_3D_vision_Final_Report.pdf" target="_blank">
                <papertitle>Sparse View Mesh Reconstruction of Plants</papertitle>
              </a>
              <br>
              Learning for 3D Vision Final Project (May 2022)
              <br>
              <a href="data/Learning_3D_vision_Final_Report.pdf" target="_blank">[PDF]</a>
              <a href="https://github.com/russelldj/leaf_reconstruction" target="_blank">[Data Repo]</a>
              <a href="https://github.com/hfreecmu/NeuS" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Applied NeuS and metalearning towards watertight mesh reconstruction of plants from sparse viewpoints. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/gan_augment.png' width=100%>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.andrew.cmu.edu/course/16-726-sp22/projects/hfreeman/project/" target="_blank">
                <papertitle>GANs for Coarse Style and Scene Data Augmentation</papertitle>
              </a>
              <br>
              Learning-Based Image Synthesis Final Project (May 2022)
              <br>
              <a href="https://www.andrew.cmu.edu/course/16-726-sp22/projects/hfreeman/project/" target="_blank">[Project Webpage]</a>
              <a href="https://github.com/hfreecmu/swapping-autoencoder-pytorch" target="_blank">[Code Repo]</a>
              <p></p>
              <p>
                Utilized Generative-Adversarial Networks and Swapping AutoEncoders for coarse style and scene data augmentation.
              </p>
            </td>
          </tr>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                <br>
              </p>
              <td align=right>
                Template from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>. Last updated: October 22nd 2025.</td>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
